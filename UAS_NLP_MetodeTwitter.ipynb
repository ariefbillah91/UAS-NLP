{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBqT2YwhxuebP8iCk/zztU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ariefbillah91/UAS-NLP/blob/main/UAS_NLP_MetodeTwitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp 'kaggle.json' ~/.kaggle\n",
        "!chmod 600 ~/.kaggle/'kaggle.json'\n",
        "!kaggle datasets download -d 'jp797498e/twitter-entity-sentiment-analysis'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDOgtxNzLyFG",
        "outputId": "c357069e-f81a-477b-d3ba-9b34a1a3d76a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/__init__.py\", line 23, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/api/kaggle_api_extended.py\", line 403, in authenticate\n",
            "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "DV3GLAsyMLzu"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_one_hot_encoded(label):\n",
        "  if label.lower() == 'positive':\n",
        "    return [1, 0, 0]\n",
        "  elif label.lower() == 'negative':\n",
        "    return [0, 1, 0]\n",
        "  else:\n",
        "    return [0, 0, 1]"
      ],
      "metadata": {
        "id": "Y9zI3ejpMN26"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_url (text):\n",
        "  # http://google.com adalah bla bla bla\n",
        "  # https://youtube.com\n",
        "  return re.sub(r'http\\S+', '', text)\n",
        "\n",
        "def remove_number(text):\n",
        "  # 123 saya blablabla\n",
        "  return re.sub(r'\\d+', '', text)\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  words = text.split() # saya mau ke batam -> [saya, mau, ke, batam]\n",
        "  new_text = \"\"\n",
        "  for word in words:\n",
        "    if word not in stopwords.words('english'):\n",
        "      new_text += word\n",
        "      new_text += \" \"\n",
        "\n",
        "  return new_text\n",
        "\n",
        "def lematized_text(text):\n",
        "  lematizer = WordNetLemmatizer()\n",
        "  words = text.split()\n",
        "  new_text = \"\"\n",
        "  for word in words:\n",
        "    lematized_word = lematizer.lemmatize(word)\n",
        "    new_text += lematized_word\n",
        "    new_text += \" \"\n",
        "  return new_text\n",
        "  def stemmed_text(text):\n",
        "  stemmer = PorterStemmer()\n",
        "  words = text.split()\n",
        "  new_text = \"\"\n",
        "  for word in words:\n",
        "    stemmed_word = stemmer.stem(word)\n",
        "    new_text += stemmed_word\n",
        "    new_text += \" \"\n",
        "  return new_text\n",
        "\n",
        "def preprocessing (text):\n",
        "  #lowercase -> Running, running, RUNNING, RunNinG\n",
        "  text = text.lower()\n",
        "\n",
        "  # remove url\n",
        "  text = remove_url(text)\n",
        "\n",
        "  # remove number\n",
        "  text = remove_number(text)\n",
        "\n",
        "  # remove stopwords\n",
        "  text = remove_stopwords(text)\n",
        "\n",
        "  # lematisasi\n",
        "  text = lematized_text(text)\n",
        "\n",
        "  # stemming\n",
        "  text = stemmed_text(text)\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "ywjN5-RQMSK4",
        "outputId": "b89e6e1b-8c06-4fbf-9036-34af95a40011"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after function definition on line 29 (<ipython-input-22-f2ad0fa9bccb>, line 30)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-f2ad0fa9bccb>\"\u001b[0;36m, line \u001b[0;32m30\u001b[0m\n\u001b[0;31m    stemmer = PorterStemmer()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# \"ini bapak budi\" -> 3 kata\n",
        "# \"ini ibu budi\" -> 3 kata\n",
        "# \"saya dan bapak ibu budi pergi ke pasar\" -> 8 kata\n",
        "# 0, 0, 0 , 0, 0\n",
        "\n",
        "def get_avg (text_list):\n",
        "  sum = 0\n",
        "  for text in text_list:\n",
        "    word_num = len(text)\n",
        "    sum = sum + word_num\n",
        "  return sum/len(text_list) # total kata di dataset / banyak kalimat yang ada"
      ],
      "metadata": {
        "id": "-8GjxpROMxEd"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "def tokenizing(text_list):\n",
        "  VOCAB_SIZE = 10000\n",
        "  tokenizer = Tokenizer(num_words = VOCAB_SIZE, oov_token='')\n",
        "  tokenizer.fit_on_texts(text_list)\n",
        "  word_index = tokenizer.word_index\n",
        "  max_length = get_avg(text_list)\n",
        "  print(max_length)\n",
        "  max_length = int(max_length)\n",
        "  print(max_length)\n",
        "  sequences = tokenizer.texts_to_sequences(text_list)\n",
        "  padded_sequences = pad_sequences(sequences, padding='pre', truncating='pre', maxlen=max_length)\n",
        "\n",
        "  with open('mytokenizer.pickle', 'wb') as file:\n",
        "    pickle.dump(tokenizer, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "  return padded_sequences"
      ],
      "metadata": {
        "id": "IFRjhSj0M1bM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = np.array(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "3YhiIXN1M243",
        "outputId": "a3832168-a907-4b00-9b69-07a39366423d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'labels' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-c628b53db1a2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=10001, output_dim=100, input_length=57),\n",
        "    tf.keras.layers.LSTM(100),\n",
        "    tf.keras.layers.Dense(100, activation = 'relu', input_shape=(None, 75)),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "NP2km7h9M-pm"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss = 'categorical_crossentropy',\n",
        "    metrics = ['accuracy'],\n",
        "    optimizer = 'adam'\n",
        ")"
      ],
      "metadata": {
        "id": "XlobqwM5NAjF"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('mymodel.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgkpJP67NE-n",
        "outputId": "04c7aec2-d83f-4a63-b1f8-9be65b5c4860"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    }
  ]
}